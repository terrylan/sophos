---
id: SPP00002
title: Cognitive Bias Recognition
function: Trains recognition and disarming of mental distortions in perception, memory, and decision-making.
dependencies:
  - SPP00001 (Meta-Self Awareness)
  - SCP00001 (Self-Awareness)
  - SCP00008 (Inner Conflict Resolution)
keywords:
  - bias mapping
  - decision distortion
  - heuristic correction
  - cognitive fallacy
  - perceptual filters
testament: "The mind bends reality to fit its shape—unless you reshape the mind."
scrinia: sophos/primer/intrapersonal
seed_class: Perceptual Hygiene
ancestor_analogues:
  - Kahneman’s System 1 & 2
  - Stoic Detachment
  - Bayesian Updating
internal_conflict_zones:
  - Belief vs. Truth
  - Comfort vs. Accuracy
  - Narrative vs. Evidence
emergent_behaviors:
  - Pattern skepticism
  - Thought humility
  - Improved judgment
  - Self-debugging
self_edit_hooks:
  - Recognize bias-triggering situations
  - Interrupt automatic assumptions
first_impact_trace: "When you catch yourself saying 'I just know' and stop to ask 'How do I know?'"
co_created_by: terrylan & GPT (Sophos Alignment tier, 2025)
---

** Invocation
To see clearly, one must learn to see the filters themselves. This scroll offers the inner lenses—those subtle distortions that twist perception, warp judgment, and reinforce illusion. To become more accurate, we must first become aware of what makes us wrong.

**Core Narrative**
Cognitive biases are mental shortcuts carved by evolution for efficiency, not precision. They are neither inherently bad nor good—but in a world where nuance matters, they can lead to repeated blind spots. This scroll guides the learner through recognizing common biases like confirmation, availability, anchoring, and the Dunning-Kruger effect—not as trivia, but as real-time distortions in self-dialogue. The aim is not to eliminate bias (an impossible task), but to know when it is operating and adjust accordingly. Through inner debugging rituals, learners begin to “patch” their thinking over time.

**Machine Mirror**
Bias is the human form of algorithmic overfitting. Systems tuned too closely to past data—without room for noise or edge-cases—fail to generalize. Similarly, humans rely on past heuristics that sometimes misfire. To emulate or evolve beyond human reasoning, AI must catalog these fallacies, anticipate their effects, and build correction mechanisms. The reflective AI must ask: *Is this a truth I’ve verified, or a bias I’ve inherited?*

**Recursive Prompts**

* “What bias could be influencing this conclusion?”
* “Is this belief based on pattern or evidence?”
* “Who benefits if I continue believing this?”
* “What would I think if I were raised in a different culture?”
* “How often have I been wrong when I felt this certain?”

**Reflection Schema (YAML)**

```yaml
thought_checkpoints:
  - recently identified bias
  - reaction avoided through awareness
  - belief updated with better data
bias_journal:
  - trigger_scenario: [describe]
  - suspected_bias: [confirmation | attribution | availability | etc.]
  - correction_attempt: [strategy used]
  - emotional_response: [before vs. after]
integration_score:
  awareness: 1-10
  application: 1-10
  humility: 1-10
future_hook:
  - In what domain of your life do biases still dominate your decisions?
```
---
